<!DOCTYPE html>
<html lang="en">
  <head><script type="text/javascript" src="/blog/lib/jquery/jquery.min.js"></script>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><script type="text/javascript" src="/blog/lib/slideout/slideout.js"></script>
  <script type="text/javascript" src="/blog/lib/fancybox/jquery.fancybox.pack.js"></script>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>


<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">

<meta name="description" content="A Comprehensive Study of Deep Video Action Recognition"/><meta name="keywords" content="行为识别, action recognition, review, survey, Onns Blog" /><link rel="alternate" href="/blog/default" title="Onns Blog"><link rel="shortcut icon" type="image/x-icon" href="/blog/favicon.ico?v=2.11.0" />
<link rel="canonical" href="https://onns.xyz/2020/12/26/a-comprehensive-study-of-deep-video-action-recognition/"/>

<link rel="stylesheet" type="text/css" href="/blog/lib/fancybox/jquery.fancybox.css" /><script type="text/javascript">
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]  
      },
      svg: {
        fontCache: 'global'
      }
    };
  </script>
  <script type="text/javascript" async src="//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<link rel="stylesheet" type="text/css" href="/blog/css/style.css?v=2.11.0" />

<script id="baidu_analytics">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?094f3776af9873aa4bf55d2700e2b1cc";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script><!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-127068305-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-127068305-1');
</script><script src="//onns.xyz/js/av-min.js"></script>
  <script id="leancloud">
    AV.init({
      appId: "jIRe5LRqbWDB2dxmu7FH8c1S-gzGzoHsz",
      appKey: "pM10kNYtPMwvqYUCWfbUGBPJ",
      serverURLs: "https://jire5lrq.lc-cn-n1-shared.com"
    });
  </script><script>
  window.config = {"leancloud":{"app_id":"jIRe5LRqbWDB2dxmu7FH8c1S-gzGzoHsz","app_key":"pM10kNYtPMwvqYUCWfbUGBPJ","server_urls":"https://jire5lrq.lc-cn-n1-shared.com"},"toc":true,"fancybox":true,"pjax":true,"latex":true};
</script>

<script id="search">
    var searchFunc = function(path, search_id, content_id) {
    'use strict';
    $.ajax({
        url: path,
        dataType: "xml",
        success: function( xmlResponse ) {
            // get the contents from search data
            var datas = $( "entry", xmlResponse ).map(function() {
                return {
                    title: $( "title", this ).text(),
                    content: $("content",this).text(),
                    url: $( "url" , this).text()
                };
            }).get();

            var $input = document.getElementById(search_id);
			if (!$input) return;
            var $resultContent = document.getElementById(content_id);
            
            $input.addEventListener('input', function () {
                var str = '<section class=\"posts\">';
                var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                $resultContent.innerHTML = "";
                if (this.value.trim().length <= 0) {
                    return;
                }
                // perform local searching
                datas.forEach(function (data) {
                    var isMatch = true;
                    var content_index = [];
                    if (!data.title || data.title.trim() === '') {
                        data.title = "Untitled";
                    }
                    var data_title = data.title.trim().toLowerCase();
                    var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                    var data_url = data.url;
                    var index_title = -1;
                    var index_content = -1;
                    var first_occur = -1;
                    // only match artiles with not empty contents
                    if (data_content !== '') {
                        keywords.forEach(function (keyword, i) {
                            index_title = data_title.indexOf(keyword);
                            index_content = data_content.indexOf(keyword);

                            if (index_title < 0 && index_content < 0) {
                                isMatch = false;
                            } else {
                                if (index_content < 0) {
                                    index_content = 0;
                                }
                                if (i == 0) {
                                    first_occur = index_content;
                                }
                                // content_index.push({index_content:index_content, keyword_len:keyword_len});
                            }
                        });
                    } else {
                        isMatch = false;
                    }
                    // show search results
                    if (isMatch) {
                        str += `
<article class="post">
    <header class="post-header">
      <h1 class="post-title"><a class="post-link" href="`+ data_url +`">`+ data_title +`</a>
        </h1>
    </header>
    <div class="post-content">
        `;
                        var content = data.content.trim().replace(/<[^>]+>/g, "");
                        if (first_occur >= 0) {
                            // cut out 100 characters
                            var start = first_occur - 20;
                            var end = first_occur + 80;

                            if (start < 0) {
                                start = 0;
                            }

                            if (start == 0) {
                                end = 100;
                            }

                            if (end > content.length) {
                                end = content.length;
                            }

                            var match_content = content.substring(start, end);

                            // highlight all keywords
                            keywords.forEach(function (keyword) {
                                var regS = new RegExp(keyword, "gi");
                                match_content = match_content.replace(regS, "<code>" + keyword + "</code>");
                            });

                            str += "<p>" + match_content + "...</p>"
                        }
                        str += "</article>";
                    }
                });
                str += "</section>";
                $resultContent.innerHTML = str;
            });
        }
    });
}    
var search_path = "search.xml";
if (search_path.length == 0) {
search_path = "search.xml";
}
var path = "/blog/" + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script>
    <title>A Comprehensive Study of Deep Video Action Recognition - Onns Blog</title>
  <meta name="generator" content="Hexo 4.2.1"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>

  <body><div id="mobile-navbar" class="mobile-navbar">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="mobile-header-logo">
    <a href="/blog/." class="logo">Onns Blog</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>

<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list"><a href="/blog/home">
        <li class="mobile-menu-item">Home
          </li>
      </a><a href="/blog/archives">
        <li class="mobile-menu-item">Archives
          </li>
      </a><a href="/blog/tags">
        <li class="mobile-menu-item">Tags
          </li>
      </a><a href="/blog/categories">
        <li class="mobile-menu-item">Categories
          </li>
      </a><a href="/blog/about">
        <li class="mobile-menu-item">About
          </li>
      </a></ul>
</nav>
<div class="container" id="mobile-panel">
      <header id="header" class="header"><div class="logo-wrapper">
  <a href="/blog/." class="logo">Onns Blog</a>
</div>

<nav class="site-navbar"><div class="search-button">
        <a href="#" class="search-toggle" data-selector=".site-navbar"></a>
    </div>
    <div class="search-box">
        <input type="text" id="local-search-input" class="text search-input" placeholder="Type here to search..." />
    </div><ul id="menu" class="menu"><li class="menu-item">
          <a class="menu-item-link" href="/blog/home">
            Home
            </a>
        </li>
      <li class="menu-item">
          <a class="menu-item-link" href="/blog/archives">
            Archives
            </a>
        </li>
      <li class="menu-item">
          <a class="menu-item-link" href="/blog/tags">
            Tags
            </a>
        </li>
      <li class="menu-item">
          <a class="menu-item-link" href="/blog/categories">
            Categories
            </a>
        </li>
      <li class="menu-item">
          <a class="menu-item-link" href="/blog/about">
            About
            </a>
        </li>
      </ul></nav>
</header>

      <main id="main" class="main">
        <div class="content-wrapper">
          <div id="content" class="content">
            <div id="local-search-result"></div><article class="post">
    <header class="post-header">
      <h1 class="post-title">A Comprehensive Study of Deep Video Action Recognition
        </h1>

      <div class="post-meta">
        <span class="post-time">
          2020-12-26 10:46:35
        </span><span class="post-category">
            <a href="/blog/categories/study/">study</a>
            </span>
        <span class="post-visits"
             data-url="/blog/2020/12/26/a-comprehensive-study-of-deep-video-action-recognition/"
             data-title="A Comprehensive Study of Deep Video Action Recognition">
          Visits 0
        </span>
        </div>
    </header>

    <div class="post-toc" id="post-toc">
    <h2 class="post-toc-title">Contents</h2>
    <div class="post-toc-content">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#a-comprehensive-study-of-deep-video-action-recognition"><span class="toc-text">A Comprehensive Study of Deep Video Action Recognition</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#挑战"><span class="toc-text">挑战</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#史诗"><span class="toc-text">史诗</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#two-stream-networks"><span class="toc-text">Two-stream networks</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#基于分段的方法"><span class="toc-text">基于分段的方法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#多流网络"><span class="toc-text">多流网络</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#三维卷积神经网络"><span class="toc-text">三维卷积神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#统一二维和三维卷积神经网络"><span class="toc-text">统一二维和三维卷积神经网络</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#提升三维卷积的效率"><span class="toc-text">提升三维卷积的效率</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#高效视频建模"><span class="toc-text">高效视频建模</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#类光流算法"><span class="toc-text">类光流算法</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#未来研究方向"><span class="toc-text">未来研究方向</span></a></li></ol></li></ol>
    </div>
  </div><div class="post-content"><h1 id="a-comprehensive-study-of-deep-video-action-recognition"><a class="header-anchor" href="#a-comprehensive-study-of-deep-video-action-recognition">#</a>A Comprehensive Study of Deep Video Action Recognition</h1>
<p>2020 年 12 月的新论文，亚马逊李沐团队提出，行为识别的全面调研（2014-2020）。</p>
<a id="more"></a>
<p>最近十年的数据集：</p>
<center>
<!-- //onns.xyz/blog/image -->
<!-- ../image -->
<p><img src="//onns.xyz/blog/image/20201226-1.png" alt="数据集"></p>
</center>
<p>行为识别研究分为三个大体的趋势：</p>
<center>
<!-- //onns.xyz/blog/image -->
<!-- ../image -->
<p><img src="//onns.xyz/blog/image/20201226-2.png" alt="时间线"></p>
</center>
<p>上方的双流/2D 卷积和下方的 3D 卷积。</p>
<p>还有一条隐藏的线是提高计算效率，使之适用于更大的数据集，从而在实际应用中得到应用。</p>
<p><code>UCF101</code>和<code>Kinetics400</code>基本上可以通过场景和内容来推断出分类结果。</p>
<p><code>Something-Something</code>数据集需要强大的时间建模，因为大多数活动不能仅基于空间特征来推断。该数据集中于人与对象的交互，因此它更细粒度，需要强大的时间建模。</p>
<h2 id="挑战"><a class="header-anchor" href="#挑战">#</a>挑战</h2>
<p>从数据集角度，为训练行为识别的模型定义标签空间是一项很大的挑战，因为一项动作可能由不同的人会由不同的定义（比如拿起杯子和喝水）。除此之前，对视频进行的行为进行标注是<code>费力的</code>（你必须观看所有的帧）和<code>模糊的</code>（一个动作什么时候酸结束，这个是很难定义的）。还有一个原因，像<code>Kinetics</code>数据集只给了视频链接，其中很大一部分的数据已经被删除或者不可访问，大家很难在同一个基准上做实验。</p>
<p>从建模角度，人类的行为动作具有很强的类内和类间差异，第二，识别人类行为需要同时理解短期特定动作的运动信息和长期时间信息。第三，训练和推理的计算成本都很高，阻碍了动作识别模型的开发和部署。</p>
<h2 id="史诗"><a class="header-anchor" href="#史诗">#</a>史诗</h2>
<p>所有的故事都会是从<code>DT</code>和<code>iDT</code>开始讲，但是它的缺点：手工制作的特性计算成本很高，而且难以扩展和部署。</p>
<p><code>DeepVideo</code>算是这一派的开山祖师，不过效果并不如上面说的手工设计的特征（在<code>UCF101</code>数据集上 <strong>65.4%</strong> vs <strong>87.9%</strong>）。不过<code>DeepVideo</code>发现，一个由单个视频帧所训练的网络，在输入被改变为一组帧时，同样表现良好。这其实表明其实网络并没有很好得捕捉动作信息。这其实也鼓励人们思考，为什么卷积神经网络模型不像其他计算机视觉任务一样，在视频领域的表现没有超过传统手工制作的特征。</p>
<h3 id="two-stream-networks"><a class="header-anchor" href="#two-stream-networks">#</a>Two-stream networks</h3>
<p>由于视频的直观理解需要运动信息，寻找合适的方式来描述帧间的时间关系对于提高基于卷积神经网络的视频动作识别的性能至关重要。</p>
<p>与使用原始 RGB 图像作为输入相比，光流可以有效地去除静止背景，从而使学习问题更加简单。</p>
<p>双流法得出了两个重要的结论：</p>
<ol>
<li>运动信息是视频动作识别的重要内容。</li>
<li>对于 CNN 来说，直接从原始视频帧中学习时间信息仍然具有挑战性。</li>
</ol>
<p>因为在一个双流网络中有两个流。将会有一个阶段，需要合并两个网络的结果，以获得最终的预测。这一阶段通常称为时空融合步骤。</p>
<p>最简单、最直接的方法是<code>late fusion</code>。它对来自两个信息流的预测进行加权平均。尽管后期融合被广泛采用，但许多研究人员认为，这可能不是融合空间外观流和时间运动流信息的最佳方式。他们相信，在模型学习过程中，两个网络之间早期的互动可以使两个流都受益，这被称为早期融合，即<code>early fusion</code>。</p>
<p>（20201227 更新：LSTM 我一窍不通，所以省略）</p>
<h4 id="基于分段的方法"><a class="header-anchor" href="#基于分段的方法">#</a>基于分段的方法</h4>
<p><code>TSN</code>在每个片段中随机选择一个视频帧，最后，分段一致性被执行以聚合来自采样视频帧的信息。<code>TSN</code>能够对长期时间结构进行建模，因为模型可以看到整个视频的内容。此外，这种稀疏采样策略降低了长视频序列的训练成本，但保留了相关信息。</p>
<h4 id="多流网络"><a class="header-anchor" href="#多流网络">#</a>多流网络</h4>
<p>双流网络之所以成功，是因为视频的<code>外观</code>和<code>运动信息</code>是两个最重要的属性。但还有一些其它的对行为识别有帮助的信息，比如<code>姿态</code>、<code>物体</code>、<code>声音</code>或者<code>深度信息</code>等。</p>
<h3 id="三维卷积神经网络"><a class="header-anchor" href="#三维卷积神经网络">#</a>三维卷积神经网络</h3>
<p><code>C3D</code>在标准基准上的性能并不令人满意，但显示出很强的泛化能力，可以作为各种视频任务的通用特征提取器。</p>
<p>然而，3D 网络很难优化。为了更好地训练一个 3D 卷积滤波器，人们需要一个包含不同视频内容和动作类别的大规模数据集。</p>
<p>然而，C3D 的训练需要几周的时间来收敛。尽管 C3D 很受欢迎，但大多数用户只是将其作为不同用例的特征提取器，而不是修改/微调网络。</p>
<p><code>I3D</code>改变了这一现象，它的主要贡献点有两个：</p>
<ol>
<li>用成熟的图像分类体系用于 3D CNN。</li>
<li>对于模型权值，采用【Towards Good Practices for Very Deep Two-Stream ConvNets】中初始化光流网络的方法，将 ImageNet 预先训练好的 2D 模型权值膨胀到 3D 模型中的对应模型。</li>
</ol>
<p>三维卷积神经网络并不会取代双流网络，它们也不是相互排斥的。</p>
<h4 id="统一二维和三维卷积神经网络"><a class="header-anchor" href="#统一二维和三维卷积神经网络">#</a>统一二维和三维卷积神经网络</h4>
<p><code>P3D</code>和<code>R2+1D</code> 都是把<code>3x3x3</code>的卷积核替换成了<code>3x1x1</code>和<code>1x3x3</code>的卷积核来减少计算量。</p>
<p>除此之外还有别的方法直接将<code>2D</code>和<code>3D</code>卷积在一个网络里进行了混合。</p>
<p>在网络的底部使用二维卷积核替代三维卷积核，发现这种头重脚轻的网络具有更高的识别精度。</p>
<p>（<code>non-local</code>是区别于卷积核卷局部的概念而提出的，但是这个方向我没有研究，所以也略过。）</p>
<h4 id="提升三维卷积的效率"><a class="header-anchor" href="#提升三维卷积的效率">#</a>提升三维卷积的效率</h4>
<p>随着高效 2D 网络的发展，研究者们开始采用<code>基于信道的可分卷积</code>，并将其扩展到视频分类中。</p>
<h3 id="高效视频建模"><a class="header-anchor" href="#高效视频建模">#</a>高效视频建模</h3>
<p>随着数据集大小的增加和部署需求的增加，效率成为一个重要的关注点。如果我们使用基于双流网络的方法，我们需要预先计算光流并存储在本地磁盘上。以<code>Kinetics-400</code>数据集为例，存储所有光流图像需要 4.5TB 的磁盘空间。如此庞大的数据量将使 I/O 成为训练过程中最紧张的瓶颈，导致 GPU 资源的浪费和实验周期的延长。此外。预计算光流并不便宜，这意味着所有的双流网络方法都不是实时的。</p>
<p>如果我们使用基于 3D cnn 的方法，人们仍然发现 3D cnn 训练困难，部署具有挑战性。在训练方面，使用高端 8-GPU 机器在 Kinetics400 数据集上训练一个标准的<code>Slow-Fast</code>网络需要 10 天才能完成。如此漫长的实验周期和巨大的计算成本，使得视频理解研究只能面向拥有丰富计算资源的大公司/实验室。最近有几次尝试加速深度视频模型的训练，但与大多数基于图像的计算机视觉任务相比，这些仍然是昂贵的。</p>
<p>在部署方面，3D 卷积在不同平台上的支持不如 2D 卷积，3D cnn 需要更多的视频帧作为输入，增加了额外的 IO 成本。</p>
<h4 id="类光流算法"><a class="header-anchor" href="#类光流算法">#</a>类光流算法</h4>
<p>双流网络的一个主要缺点是对光流的需求。预计算光流的计算成本高，存储要求高，而且不能对视频动作识别进行端到端训练。如果我们能找到一种不用光流来编码运动信息的方法，那将是很有吸引力的。至少在推理时间。</p>
<p><code>PAN</code>通过计算连续特征图之间的差异。</p>
<h2 id="未来研究方向"><a class="header-anchor" href="#未来研究方向">#</a>未来研究方向</h2>
<ol>
<li>模型分析和可视化</li>
<li><strong>数据增强</strong></li>
<li>视频领域自适应（迁移学习）</li>
<li>神经结构搜索</li>
<li><strong>高效的模型开发</strong></li>
<li>新数据集</li>
<li><strong>视频对抗攻击</strong></li>
<li><strong>零样本动作识别</strong></li>
<li>弱监督的视频动作识别</li>
<li>细粒度视频动作识别</li>
<li>以自我为中心的行为识别</li>
<li>多模态</li>
<li><strong>自监督视频表示学习</strong></li>
</ol>

      </div>
      
      <footer class="post-footer">
        <div class="post-tags">
            <a href="/blog/tags/%E8%A1%8C%E4%B8%BA%E8%AF%86%E5%88%AB/">行为识别</a>
            <a href="/blog/tags/action-recognition/">action recognition</a>
            <a href="/blog/tags/review/">review</a>
            <a href="/blog/tags/survey/">survey</a>
            </div>
        
        <nav class="post-nav"><a class="prev" href="/blog/2020/12/27/interim-report/">
        <i class="iconfont icon-left"></i>
        <span class="prev-text nav-default">当前工作进度报告</span>
        <span class="prev-text nav-mobile">Prev</span>
      </a>
    <a class="next" href="/blog/2020/12/19/diverse-temporal-aggregation-and-depthwise-spatiotemporal-factorization-for-efficient-video-classification/">
        <span class="next-text nav-default">Diverse Temporal Aggregation and Depthwise Spatiotemporal Factorization for Efficient Video Classification</span>
        <span class="prev-text nav-mobile">Next</span>
        <i class="iconfont icon-right"></i>
      </a>
    </nav></footer>
    </article></div><div class="comments" id="comments"><div id="gitalk-container"></div>
    </div></div>
      </main>

      <footer id="footer" class="footer"><div class="social-links"><a href="mailto:onns@onns.xyz" class="iconfont icon-email" title="email"></a>
        <a href="https://facebook.com/onnsxyz" target="_blank" rel="noopener" class="iconfont icon-facebook" title="facebook"></a>
        <a href="https://github.com/onns" target="_blank" rel="noopener" class="iconfont icon-github" title="github"></a>
        <a href="https://www.douban.com/people/onnsxyz/" target="_blank" rel="noopener" class="iconfont icon-douban" title="douban"></a>
        <a href="https://instagram.com/onnsxyz" target="_blank" rel="noopener" class="iconfont icon-instagram" title="instagram"></a>
        <a href="/blog/atom.xml" class="iconfont icon-rss" title="rss"></a>
    </div><div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/ahonn/hexo-theme-even" target="_blank" rel="noopener">Even</a>
  </span>

  <span class="copyright-year">&copy;2015 - 2022<span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">Onns</span>
    <span class="licence"> <a href="http://www.beian.miit.gov.cn" target="_blank" rel="noopener">闽ICP备15022938号-2</a> </span>
  </span>
</div>
</footer>

      <div class="back-to-top" id="back-to-top">
        <i class="iconfont icon-up"></i>
      </div>
    </div>
<script src="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>


<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css"/>


<script src="//cdn.jsdelivr.net/npm/js-md5@0.7.3/src/md5.min.js"></script>

<script>
  var gitalk = new Gitalk({
    clientID: '7ebadcd52312c2b34f81',
    clientSecret: '052f4cb2121b344c1acd459e277d1bcdc53d1bce',
    repo: 'onns.xyz',
    owner: 'onns',
    admin: ['onns'],
    id: md5(location.pathname),
    
      language: window.navigator.language || window.navigator.userLanguage,
    
    distractionFreeMode: 'true'
  });
  gitalk.render('gitalk-container');
</script><script type="text/javascript" src="/blog/js/src/even.js?v=2.11.0"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>
